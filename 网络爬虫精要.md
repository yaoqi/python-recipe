网络爬虫是一种按照一定的规则，自动地抓取网站信息的程序或者脚本。

本文以Python语言为例简要谈谈爬虫是如何编写的。

<!--more-->

## 如何爬取网站信息

写爬虫之前，我们必须确保能够爬取目标网站的信息。

不过在此之前必须弄清以下三个问题:

1. 网站是否已经提供了api
2. 网站是静态的还是动态的
3. 网站是否有反爬的对策

### 情形1：开放api的网站

一个网站倘若开放了api，那你就可以直接GET到它的json数据。

比如[xkcd的about页](https://xkcd.com/about/)就提供了api供你下载

``` python
import requests
requests.get('https://xkcd.com/614/info.0.json').json()
```

那么如何判断一个网站是否开放api呢？有3种方法：

1. 在站内寻找api入口
2. 用搜索引擎搜索“某网站 api”
3. 抓包。有的网站虽然用到了ajax（比如[果壳网](https://www.guokr.com/scientific/)的瀑布流文章，亦或是[unsplash](https://unsplash.com/)的瀑布流图片），但是通过抓包还是能够获取XHR里的json数据的，不要傻乎乎地去用selenium，反而会降低效率。

> 怎么抓包：F12 - Network - F5刷新

实际上，app的数据也可以通过抓包来获取。

#### app抓包

安装[fiddler](www.telerik.com/fiddler)并启动，打开Tools-Options-Connections，将Allow remote computers to connect打上勾并重启fiddler。

命令行上输入ipconfig，查看自己网络的ipv4地址，在手机的网络上设置HTTP代理，端口为8888。

这时虽说能抓到数据，但都是HTTP的，而app的大部分数据都是HTTPS的。

在Options-HTTPS中将Decrypt HTTPS traffic打上勾。

以ios系统为例，在Safari浏览器中输入http://ipv4:8888，下载证书并安装。

这样就能抓到HTTPS数据啦！

### 情形2：不开放api的网站

如果此网站是静态页面，那么你就可以解析它的HTML。

解析库强烈推荐[parsel](https://parsel.readthedocs.org/)，不仅语法和css选择器类似，而且速度也挺快，Scrapy用的就是它。

> 你需要了解一下[css选择器的语法](http://www.w3school.com.cn/cssref/css_selectors.asp)（[xpath](http://www.w3school.com.cn/xpath/xpath_syntax.asp)也行），并且学会看网页的审查元素

比如获取konachan的所有原图链接

``` python
from parsel import Selector
res = requests.get('https://konachan.com/post')
tree = Selector(text=res.text)
imgs = tree.css('a.directlink::attr(href)').extract()
```

如果此网站是动态页面，先用selenium来渲染JS，再用HTML解析库来解析driver的page_source。

比如获取hitomi.la的数据（这里把chrome设置成了无头模式）

``` python
from selenium import webdriver
options = webdriver.ChromeOptions()
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)
driver.get('https://hitomi.la/type/gamecg-all-1.html')
tree = Selector(text=driver.page_source)
gallery_content = tree.css('.gallery-content > div')
```

### 情形3：反爬的网站

目前的反爬策略常见的有：验证码、登录、封ip等。

验证码：利用打码平台破解（如果硬上的话用opencv或keras训练图）

登录：利用requests的post或者selenium模拟用户进行模拟登陆

封ip：买些代理ip（免费ip一般都不管用），requests中传入proxies参数即可

其他防反爬方法：伪装User-Agent，禁用cookies等

## 如何编写结构化的爬虫

爬虫的结构很简单，无非就是创造出一个tasklist，对tasklist里的每一个task调用crawl函数。

> 大多数网页的url构造都是有规律的，你只需根据它用列表推倒式来构造出tasklist
>
> 对于那些url不变的动态网页，先考虑抓包，不行再用selenium点击下一页

如果追求速度的话，可以考虑用concurrent.futures或者asyncio等库。

``` python
import requests
from parsel import Selector
from concurrent import futures

domain = 'https://www.doutula.com'

def crawl(url):
    res = requests.get(url)
    tree = Selector(text=res.text)
    imgs = tree.css('img.lazy::attr(data-original)').extract()
    # save the imgs ...


if __name__ == '__main__':
    tasklist = [f'{domain}/article/list/?page={i}' for i in range(1, 551)]
    with futures.ThreadPoolExecutor(50) as executor:
        executor.map(crawl, tasklist)
```

数据存储的话，看需求，存到数据库中的话只需熟悉对应的驱动即可。

常用的数据库驱动有：[pymysql](https://github.com/PyMySQL/PyMySQL)(MySQL), [pymongo](https://github.com/mongodb/mongo-python-driver)(MongoDB)

## 框架

读到这里，相信你已经对网络爬虫的结构有了个清晰的认识，可以去上手框架了。

[looter](https://github.com/alphardex/looter)是本人写的一个轻量级框架，适合中小型项目；比较大型的项目建议用[scrapy](https://github.com/scrapy/scrapy)。